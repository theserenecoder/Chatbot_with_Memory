{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799c4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd468c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001EE06F17990>, default_metadata=(), convert_system_message_to_human=True, model_kwargs={})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## defining LLM\n",
    "## The model name is set to \"gemini-2.0-flash\" for the latest Gemini 2.0 model.\n",
    "## The convert_system_message_to_human flag is set to True to ensure that system messages are treated as human messages.\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash\", convert_system_message_to_human=True)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7defdb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ash! It's nice to meet you. Being a GenAI Engineer sounds like a fascinating and cutting-edge field. \\n\\nWhat kind of GenAI projects are you working on these days? I'm always interested in learning more about the applications and challenges in the field.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-12c9e412-45cb-491d-a640-827ccd3c8646-0', usage_metadata={'input_tokens': 14, 'output_tokens': 59, 'total_tokens': 73, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The human message is set to \"Hi, my name is Ash and I am a GenAI Engineer\" to introduce the user.\n",
    "llm.invoke([HumanMessage(content = \"Hi, my name is Ash and I am a GenAI Engineer.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a272b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Ash, and you are a GenAI Engineer.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-a1bf050d-adf8-4460-9ca2-90675585a103-0', usage_metadata={'input_tokens': 132, 'output_tokens': 14, 'total_tokens': 146, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Invoke the LLM with a list of messages\n",
    "# The LLM will respond to the last HumanMessage in the list\n",
    "llm.invoke(\n",
    "   [\n",
    "       HumanMessage(content=\"Hi, my name is Ash and I am a GenAI Engineer\"),\n",
    "       AIMessage(content=\"Hi Ash, nice to meet you! It's great to connect with a fellow GenAI Engineer. The field is incredibly exciting and rapidly evolving.\\n\\nWhat kind of GenAI projects are you working on? I'm always interested in learning about what others are doing in the space. Are you focusing on specific modalities (text, image, audio, etc.) or applications?\\n\\nLet me know if there's anything I can help you with, whether it's brainstorming ideas, sharing resources, or just discussing the latest trends.\"),\n",
    "       HumanMessage(content=\"Hey what is my name and what do I do?\")\n",
    "   ] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e72a3f",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep the track of inputs and outputs of the model, and store them in some data source. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a185b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "## create a dictionary to store the chat message history for each session ID\n",
    "## In a real-world application, you might want to use a more robust storage solution (e.g., a database).\n",
    "store = {}\n",
    "\n",
    "## create a function to get the chat message history for a given session ID\n",
    "## This function will be used to retrieve the message history for a given session ID.\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    \"\"\" Get the chat message history for a given session ID.\n",
    "    If the session ID does not exist, create a new one.\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "## Create a RunnableWithMessageHistory instance\n",
    "## This will allow you to use the LLM with message history.\n",
    "## The get_session_history function will be used to retrieve the message history for a given session ID.\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a4e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new session and add messages to it\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "483a7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invoke the LLM with message history\n",
    "## The session ID is passed in the config dictionary.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is Ash and I am a GenAI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfbd056a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is Ash and I am a GenAI Engineer', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Ash, it's great to meet you! It's exciting to connect with another GenAI Engineer. What kind of projects are you working on currently? I'm always interested in learning about the innovative applications of generative AI.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-55d61d5d-5e5f-4fc5-9bcb-55ac2ee48302-0', usage_metadata={'input_tokens': 13, 'output_tokens': 49, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check what get_session_history returns\n",
    "get_session_history(\"chat1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd14ce9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ash, it's great to meet you! It's exciting to connect with another GenAI Engineer. What kind of projects are you working on currently? I'm always interested in learning about the innovative applications of generative AI.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The response will include the message history for the session ID \"chat1\".\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561348c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Ash.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-75a494dc-1601-4d43-b081-bb4c3d8152f2-0', usage_metadata={'input_tokens': 65, 'output_tokens': 6, 'total_tokens': 71, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can see that the message history is being used to keep track of the conversation.\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae807ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I have no memory of past conversations. Therefore, I don't know your name. You haven't told me!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now we will change the session id to \"chat2\" and see if the message history is being used.\n",
    "config2 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "## Invoke the LLM with message history\n",
    "## The session ID is passed in the config dictionary.\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config = config2\n",
    ")\n",
    "\n",
    "## We can see that the message history is not being used for the new session ID \"chat2\".\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191de24",
   "metadata": {},
   "source": [
    "## Using Prompt Template\n",
    "\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but sill taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30612460",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will create a prompt template that uses the message history.\n",
    "## We will use the ChatPromptTemplate class to create a prompt template.\n",
    "## We will use the MessagesPlaceholder class to dynamically insert the message history into the prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all the questions to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91436f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ashutosh, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-4f593a97-779f-4b58-95e8-49ad2c1d2bee-0', usage_metadata={'input_tokens': 25, 'output_tokens': 21, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## As we have included the MessagePlaceholder we will provide the messages as key-value pair in the invoke method.\n",
    "## The key is the variable name we provided in the prompt template.\n",
    "chain.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Hi, my name is Ashutosh\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c045b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use the RunnableWithMessageHistory with the chain.\n",
    "## This will allow us to keep track of the message history while using the chain.\n",
    "with_message_history2 = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a168a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will use the RunnableWithMessageHistory with the chain.\n",
    "## We will use the session ID \"chat3\" for this example.\n",
    "config3 = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "\n",
    "## Invoke the LLM with message history\n",
    "## The session ID is passed in the config dictionary.\n",
    "response = with_message_history2.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is Ashutosh and I am a GenAI Engineer\")],\n",
    "    config=config3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c543449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ashutosh, it's nice to meet you! As a GenAI Engineer, you're working in a really exciting and cutting-edge field. How can I help you today? I'm ready to answer your questions, assist with brainstorming, or provide information to the best of my abilities. Let me know what you need!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c02ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding more complexity to the prompt template.\n",
    "## We will add a language variable to the prompt template.\n",
    "prompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all the questions to the best of your ability in {language}.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain2 = prompt2 | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0624b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will add a language variable to the invoke method.\n",
    "## The key is the variable name we provided in the prompt template.\n",
    "response = chain2.invoke(\n",
    "    {\n",
    "        \"messages\":[HumanMessage(content = \"Hi my name is Ashutosh.\")],\n",
    "        \"language\":\"Hindi\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6f2f23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते आशुतोष, मैं आपकी कैसे मदद कर सकता हूँ?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7ebfa",
   "metadata": {},
   "source": [
    "Now we will wrap this chain in a Message History class. As we have multiple keys in the input, we need to specify the correct key to use and save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccdcf799",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use input_messages_key to specify the key for the messages in the invoke method.\n",
    "## This tells the RunnableWithMessageHistory where to find the new message in the input. We can use any key name we want, should not be same as the key in the prompt template.\n",
    "## In this case, we will use \"messages\" as the key for the messages in the invoke method.\n",
    "with_message_history3 = RunnableWithMessageHistory(\n",
    "    chain2,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e112f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'गूगल द्वारा लॉन्च किया गया नया LLM मॉडल **Gemini 1.5 Pro** है।'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We will use the session ID \"chat4\" for this example.\n",
    "config4 = {\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response = with_message_history3.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What is the new LLM model launched by Google.\")],\n",
    "     \"language\":\"Hindi\"},\n",
    "     config=config4\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ac053",
   "metadata": {},
   "source": [
    "## Managing Coversation History\n",
    "\n",
    "One of the important concept to understand when building chatbots is how to manage conversation history. If not managed properly, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are padding in.\n",
    "\n",
    "We will be using **trim_message** helper to reduce the number of messages we are sending to our model. This trimmer allow us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and wheather to allow partial messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f264600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "## The trim_messages function is used to limit the number of tokens in the message history.\n",
    "## The max_tokens parameter is set to 200, which means that the message history will be trimmed to 200 tokens.\n",
    "## The strategy parameter is set to \"last\", which means that the last messages will be kept in the message history.\n",
    "## The include_system parameter is set to True, which means that the system messages will be included in the message history.\n",
    "## The token_counter parameter is set to llm, which means that the token counter will be the LLM itself.\n",
    "## The allow_partial parameter is set to False, which means that partial messages will not be allowed.\n",
    "## The start_on parameter is set to \"human\", which means that the message history will start with the human message.\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=200,\n",
    "    strategy=\"last\",\n",
    "    include_system=True,\n",
    "    token_counter = llm, \n",
    "    allow_partial=False,\n",
    "    start_on = \"human\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6752a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will be our messages\n",
    "messages = [\n",
    "    SystemMessage(content = \"Youa are a helpful assistant.\"),\n",
    "    HumanMessage(content = \"Hi, my name is Ash and I am a GenAI Engineer\"),\n",
    "    AIMessage(content=\"Hi Ash, nice to meet you!\"),\n",
    "    HumanMessage(content = \"I like to play football.\"),\n",
    "    AIMessage(content = \"That's great! Football is a fun sport. Do you play it often?\"),\n",
    "    HumanMessage(content = \"Yes, I play it every weekend.\"),\n",
    "    AIMessage(content = \"That's awesome! Playing sports is a great way to stay active and have fun. Do you play in a team or just for fun?\"),\n",
    "    HumanMessage(content = \"I play in a team. We have a match every Saturday.\"),\n",
    "    AIMessage(content = \"That sounds exciting! Playing in a team adds a whole new level of fun and competition. How's your team doing? Are you winning a lot of matches?\"),\n",
    "    HumanMessage(content = \"We are doing great! We have won 5 matches in a row.\"),\n",
    "    AIMessage(content = \"That's impressive! Winning 5 matches in a row is no small feat. It sounds like you and your team are really in sync and playing well together. Keep up the great work!\"),\n",
    "    HumanMessage(content = \"Thanks! We are really enjoying it.\"),\n",
    "    AIMessage(content = \"That's the most important part! Enjoying the game and having fun with your teammates is what it's all about. Do you have any favorite players or teams that you look up to?\")\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960bc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Youa are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I play in a team. We have a match every Saturday.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That sounds exciting! Playing in a team adds a whole new level of fun and competition. How's your team doing? Are you winning a lot of matches?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='We are doing great! We have won 5 matches in a row.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's impressive! Winning 5 matches in a row is no small feat. It sounds like you and your team are really in sync and playing well together. Keep up the great work!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks! We are really enjoying it.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's the most important part! Enjoying the game and having fun with your teammates is what it's all about. Do you have any favorite players or teams that you look up to?\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we will use the trim_messages function to trim the messages to 200 tokens.\n",
    "## We can see that the messages are trimmed to the last 200 tokens.\n",
    "## This helps us maintain the context of the conversation while keeping the message history manageable.\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c0191",
   "metadata": {},
   "source": [
    "### Passing Trimmer in a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc74ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you won 5 matches in a row!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "## We will use the RunnablePassthrough to assign the messages to the input of the chain.\n",
    "## The assign method is used to assign the messages to the input of the chain.\n",
    "## The itemgetter function is used to get the messages from the input.\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer) \n",
    "    | prompt2\n",
    "    | llm\n",
    ")\n",
    "\n",
    "## We will invoke the chain with the messages and the language variable.\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages +[HumanMessage(content = \"How many matches did we won?\")],\n",
    "        \"language\":\"English\"        \n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147832a",
   "metadata": {},
   "source": [
    "### Wrapping Trimmer in the Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ff06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's wrap this in the message History\n",
    "## We will use the RunnableWithMessageHistory to keep track of the message history while using the chain.\n",
    "with_message_history4 = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"messages\"\n",
    ")\n",
    "\n",
    "config5 = {\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "347aa0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history4.invoke(\n",
    "    {\n",
    "        \"messages\":[HumanMessage(content=\"Hi how many messages did I won.\")],\n",
    "        \"language\":\"English\"\n",
    "    },\n",
    "    config=config5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc26834a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not have access to your personal messages or any information about your activity. Therefore, I cannot tell you how many messages you won.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42adfa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
